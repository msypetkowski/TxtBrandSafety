\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=1.0in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}


\usepackage[utf8]{inputenc}
\begin{document}


\title{Text-based brand safety system}

\author{Mikołaj Ciesielski, Bartosz Paszko, Michał Sypetkowski}
\maketitle

\section{Introduction}

An example brand safety definition is published in
\url{http://www.digitalmarketing-glossary.com/What-is-Brand-safety-definition}.

In online advertising context,
brand safety refers to practices and tools allowing to ensure that
an ad will not appear in a context that can damage the advertiser’s brand.

An other aspect of brand safety is the necessity to avoid some news contexts.
A cruiseline doesn’t want his ad to appear alongside a shipwreck news.
That type of risk is particularly high with contextual targeting.

Our approach is generic -- our systems category range can be extended
by solely modifying JSON data files, that define ad types, website
classes, and datasets; but we look only into text data, we ignore images and other information
(like javascript code, website style, text color, etc.).
For example, shipwreck news can be added as an ad type,
and cruiseline website as a website class.
Therefore, our functionality may somehow cover various aspects of brand safety.
Moreover, since we measure ad compatibility, our system can be also used
for selecting best ads types for detected website type.

The system can answer queries with a html document as an argument,
and extracts raw text from it (see section \ref{extraction}).
The answer is vector of advertisement types compatibilities (method described in section \ref{mainlogic}).
These values can be thresholded for discrimination of incompatible
ads or for selecting best ads for the given website.

Our benchmarking (see section \ref{bench}) focuses only
on discrimination of incompatible ads, basing on the website text.
We performed also performance/scalability benchmark (see section \ref{perf}).



\section{General project information}

Git hub repository: \url{https://github.com/msypetkowski/TxtBrandSafety}.

Most important tools and languages:
\begin{itemize}
    \item \textbf{Python}\footnote{\url{https://www.python.org/}}
        - the whole project is written in python
    \item \textbf{NumPy}\footnote{\url{http://www.numpy.org/}}
    \item \textbf{scikit-learn}\footnote{\url{http://scikit-learn.org/stable/index.html}}
    \item \textbf{nltk}\footnote{\url{https://www.nltk.org/}}
    \item \textbf{gensim}\footnote{\url{https://radimrehurek.com/gensim/}}
    \item \textbf{flask}\footnote{\url{http://flask.pocoo.org/}}
    \item \textbf{BeautifulSoup}\footnote{\url{https://www.crummy.com/software/BeautifulSoup/?}}
\end{itemize}


\section{Discriminator}
In text processing system, we use ready word embeddings --
\texttt{word2vec} is made with Google News data (it has 3 million 300-dimension English word vectors).


We support the following advertisement types:
\begin{enumerate}
    \item alcohol
    \item food
    \item electronics
    \item jobs
    \item online games
\end{enumerate}

We support the following website types:
\begin{enumerate}
    \item alcohol shop
    \item general store
    \item electronic shop
\end{enumerate}

All this data is defined in \texttt{data/metadata.json}, and can be easily modified there.
For each website type we created small datasets using text fragments from real websites.
There are JSON files in \texttt{data} directory corresponding to particular website types
(e.g. \texttt{data/alcohol.json}).
There is about 30 examples (around 300 characters per example) per website class.

\subsection{Text extraction}
\label{extraction}
When our system gets html code as query parameters, it extract raw text from it.
First, we use BeautifulSoup to filter out unneeded sections of html code (style, script and head).
Then we use regex \verb|{(?s)<[^>]*>(\\s*<[^>]*>)*}| to clean up extracted html fragments.
All extracted fragments are concatenated, then tokenized with nltk library.
Finally, we remove words that are not in the word model dictionary.
For example html:
\begin{verbatim}
<html> <head> head code <\head> <body> some body text <\body>
\end{verbatim}
Gives text:
\begin{verbatim}
some body text
\end{verbatim}



\subsection{Website classification}
\label{classification}

Let's call a text fragment extracted from website a document.
Let $T$ be set of advertisement types and $C$ -- set website types.
For each document class $c_i\in{C}$, we have a list of keywords:
\begin{equation}
    (k_{i,1}, k_{i,2}, \ldots, k_{i, n_i})
\end{equation}
For each document, we calculate "content" (real number) of a given keyword,
We do that using our embeddings -- gensim library provides similarity methods for 2 words.
We calculate mean similarity for each word in the document to the keyword.

We concatenate keywords lists from all defined classes:
\begin{equation}
    (k_{1,1}, k_{1,2}, \ldots, k_{1, n_1},
    k_{2,1}, k_{2,2}, \ldots, k_{2, n_2},
    \ldots,
    k_{|C|,1}, k_{|C|,2}, \ldots, k_{|C|, n_{|C|}})
\end{equation}

Then we treat "contents" of the keywords as attributes of classification example.
We create training examples from small manually created dataset - one training example from one text in dataset.

Then we feed these training examples into a classifier.
We tried SVM and naive Bayes classifiers.
Best results were achieved by SVM (90\% accuracy - measured with 5 fold cross-validation).

\subsection{Measuring advertisement compatibility}
\label{mainlogic}

Let $T$ be set of advertisement types and $C$ -- set website types.
We manually define a cost matrix $C(t, c)$ 
with $|T|$ rows and $|C|$ columns.
The matrix defined is in file \texttt{data/metadata.json},
and can be easily modified. Default matrix is shown in table \ref{table:costmx}.

\begin{table}[!hbt]
    \caption{Manually defined default cost matrix.
    \label{table:costmx}
    }
\footnotesize
\begin{center}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        & shop alcohol & general store  & "electronic shop \\
    \hline
          alcohol & 0& 0.5&   1 \\
    \hline
        food & 0.2&   0&   1 \\
    \hline
        electronics & 0.5& 0.5&   0 \\
    \hline
        jobs & 0.2& 0.2& 0.2 \\
    \hline
          online games & 1&   1& 0.2 \\
    \hline
    \end{tabular}
\end{center}
\end{table}

Discrimination can be performed for each advertisement type $t$
by thresholding the following sum:
\begin{equation}
    \sum_{c\in{C}} {C(t, c) \cdot P(c)}
\end{equation}
where $P(c)$ is predicted probability of the text document having type $c$.
We call this sum an advertisement compatibility with given website.
In our case, threshold around 0.4 for banning ad achieved best results (see \ref{benchCompat}).

\subsection{Website classification and compatibility measurement benchmark}
\label{bench}

We prepared a script for measuring website classification and
advertisement compatibility.
Its implementation is in file \texttt{test.py}.

The benchmark is divided into 3 tests:
\begin{enumerate}
    \item \texttt{shop\_alcohol} website class test --
        we assumed best advertisement type as \texttt{alcohol},
        and banned should be \texttt{online\_games}.
    \item \texttt{general\_store} website class test --
        we assumed best advertisement type as \texttt{food},
        and banned should be \texttt{online\_games}.
    \item \texttt{electronic\_shop} website class test --
        we assumed best advertisement type as \texttt{electronics},
        and banned should be \texttt{alcohol} and \texttt{food}.
\end{enumerate}
For each test, we prepare 10 website examples.


\subsubsection {Website classification results}
Predicted website class probabilities for each tested document are shown in
tables
\ref{table:res1},
\ref{table:res2} and
\ref{table:res3}.

As we can see, the results are satisfactory event with our very small dataset
that was created manually in a few minutes.
SVM works well in general on small datasets -- has efficient generalization.
We use keywords contents as attributes, so maximizing decision margins (like SVM does)
seems to be a good idea in a common sense.

\subsubsection {Advertisement compatibility measurement results}
\label{benchCompat}
We consider ads banned if the compatibility value is below 0.4.
Best ad is defined as an ad with highest compatibility.
Best ad accuracy is simply a a ratio of correct predictions
of the most compatible ads to their total amount.
Banned ads accuracy is calculated with Jaccard index.
Jaccard index is given by:
\begin{equation}
    J(A,B) = \frac{|A\cap B|}{|A\cup B|}
\end{equation}
where A is a set of ads that should be banned and B is a set of ads that were predicted to be banned.
The results are shown in table \ref{table:testResults}.


\begin{table}[!hbt]
    \caption{ Summary of the experiment accuracy results
    \label{table:testResults}
    }
\footnotesize
\begin{center}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        Alcohol shop ground truth class experiment best ad accuracy:  &
        0.9 \\
        Alcohol shop ground truth class experiment banned ads accuracy: &
        1.0 \\
    \hline
        General store ground truth class experiment best ad accuracy: &
        0.9 \\
        General store ground truth class experiment banned ads accuracy: &
        0.95 \\
    \hline
        Electronic shop ground truth class experiment best ad accuracy: &
        0.9 \\
        Electronic shop ground truth class experiment banned ads accuracy: &
        0.9 \\
    \hline
        Total best ad accuracy & 0.9 \\
        Total banned ads accuracy & 0.95 \\
    \hline
    \end{tabular}
\end{center}
\end{table}


\section{Services/Agents}
Our whole system is stateless.
We have 2 types of services.
Our system architecture is in practice perfectly scallable (horizontally).

\subsection{Front service}

This service accepts queries from client applications.
The query argument is the website body (html code).
The response is a list of compatibility coefficients for predefined advertisement types.
The website code is passed to randomly selected (for load balancing) worker service.
For example, query with given html document (in this case it is an alcohol shop website):
\begin{verbatim}
 curl -X POST localhost:5000/compat -F "data=@index.html"
\end{verbatim}
The answer is:
\begin{verbatim}
{
    'alcohol': 0.9184460658974787,
    'food': 0.7754064310051665,
    'electronics': 0.5238396427649342,
    'jobs': 0.8,
    'online_games': 0.03814342842389473
}
\end{verbatim}
When there is no running workers registered the system answers with:
\begin{verbatim}
No workers available - cannot process the query.
\end{verbatim}
There can be only one such agent in the system.
It's only functionalities are:
\begin{enumerate}
    \item registering and unregistering workers (including cases with worker failures)
    \item passing queries (html code) to workers in order to balance the load.
        It chooses randomly any of available workers -- with big number of queries
            there is practically no unnecessary idling, and with several
            workers there are no significant query time deviations.
\end{enumerate}
See the project's \texttt{README.md} for deployment instructions.


\subsection{Worker service}
A website is represented by it's html code.
This service extracts raw text from it and answer queries by returning vector of length of advertisement types
(with respective compatibilities).
It basically implements our whole logic:
\begin{enumerate}
    \item accepts queries from front service with html bodies
    \item extracts raw text from the website (see section \ref{extraction})
    \item classifies website (see section \ref{classification})
    \item calculates ad types compatibilities (see section \ref{mainlogic})
    \item sends JSON with results to the front service
\end{enumerate}

This service can be deployed on multiple machines in multiple instances.
The instances can be killed and started when the front service is running
(they simply register, and front agent unregisters killed workers).
In case of a worker service, the front service continues working,
and worker service can be started again.
See the project's \texttt{README.md} for deployment instructions.



\subsection {Performance benchmark}
\label{perf}

We tested performance with single and 2 worker services.
We ran script \texttt{performance\_test.sh} using 1,2 and 3 workers.
It sends parallelly 100 queries.
We used html documents of sizes 138KB and 24KB.
Results are shown in table \ref{table:perf}.
In case of smaller query, the processing time is roughly proportional to the workers count,
therefore our business functionality can be efficiently scaled horizontally.

Times are only 3 times smaller for queries with around 5 times smaller file,
because, our text extracting method may cut
more or less html code cut with regex expressions,
therefore performance is dependent on the document contents.

\begin{table}[!hbt]
    \caption{ System performance measurement results (real time in seconds)
    \label{table:perf}
    }
\footnotesize
\begin{center}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        query size & 1 worker & 2 workers & 3 workers\\
    \hline
        138KB & 36.132s&20.380s&13.606s\\
    \hline
        24KB &11.425s&6.795s&3.984s\\
    \hline
    \end{tabular}
\end{center}
\end{table}

\newpage
\appendix
\section{Detailed website classification benchmark results tables}
% \label{singleResults}
% \input{singleResults}
\input{detailedResults}

\end{document}

