\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=1.0in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}


\usepackage[utf8]{inputenc}
\begin{document}


\title{Text-based brand safety system - realization project}

\author{Mikołaj Ciesielski, Bartosz Paszko, Michał Sypetkowski}
\maketitle

\section{General information}

We are implementing a system
that provides brand safety functionality.

Most important tools and languages:
\begin{itemize}
    \item \textbf{Python}\footnote{\url{https://www.python.org/}}
        - the whole project will be written in python
    \item \textbf{pykka}\footnote{\url{https://www.pykka.org/en/latest/}}
        - for communication between services
    \item \textbf{NumPy}\footnote{\url{http://www.numpy.org/}}
\end{itemize}


\section{Discriminator}
In text processing system, we will use ready word embeddings
from a model trained on large dataset (eventually we will train the embeddings).

We will manually define a fixed number of possible advertisement types.
Similarly, we will define fixed number of possible website types.
For each website type we will create small datasets
using text fragments from real websites.
Let $T$ by set of advertisement types and $C$ -- set website types.

\subsection{Website classification}

Let's call a text fragment extracted from website a document.
For each document class $c_i\in{C}$, we will create a list of keywords:
\begin{equation}
    (k_{i,1}, k_{i,2}, \ldots, k_{i, n_i})
\end{equation}
For each document, we are able to calculate "content" (real number) of a given keyword $k$,
using word embeddings.
We concatenate keywords lists from all defined classes:
\begin{equation}
    (k_{1,1}, k_{1,2}, \ldots, k_{1, n_1},
    k_{2,1}, k_{2,2}, \ldots, k_{2, n_2},
    \ldots,
    k_{|C|,1}, k_{|C|,2}, \ldots, k_{|C|, n_{|C|}})
\end{equation}
Then we treat "contents" of these keywords as attributes of classification example.
We create training examples from the dataset - one training example from one text in dataset.

Then we will feed these training examples into a classifier.
We are planning to use e.g. naive bayes classifier.

One website may contain many text fragments.
We will average class vectors from classifier,
weighting them by the text length
(or consider more sophisticated tehniques).


\subsection{Measuring advertisement compatibility}

Wi will manually define a cost matrix $C(t, c)$ 
with $|T|$ rows and $|C|$ columns.

We will perform discrimination for each advertisement type $t$
by thresholding the following sum:
\begin{equation}
    \sum_{c\in{Y}} {C(t, c) \cdot P(c)}
\end{equation}
where $P(c)$ is predicted probability of the text document having type $c$.



\section{Services types}
We decided to distinguish 2 types of services.

\subsection{Main service}
This service will accept queries from client applications.
The query argument will be the website body (html code).
Services of this type will respond with list of decision for each advertisement type
(whether the advertisement of given type conflict with the given website).
The website code is passed to a text processing service.
The service will perform load balancing by employing proper text processing services
(e.g. by random choice).


\subsection{Text extraction and processing service}
A website will be represented by it's whole html code.
This service will obtain raw text fragments from it.
This service will answer queries by returning vector of length of advertisement types.
The vector will contain probabilities whether placing an advertisement of given type
on the website (that query is about) is appriopriate.

We are planning to use GPU acceration for faster text processing.


\end{document}
